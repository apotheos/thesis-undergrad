\section{Analysis}

This research uses NLTK's implementation of a \naive Bayes classifier to determine the efficacy of using document classifiers for automatic textbook indexing, as established in section~\ref{sec:indexing-methods}.
In order to function properly, a \naive Bayes classifier requires well-defined training, test, class, and features sets.
Sections \ref{subsec:training-set} and \ref{subsec:test-set} outline the training set of Wikipedia article paragraphs and test set of {\it Biology} textbook paragraphs respectively, while section~\ref{subsec:reducing} described the set of classes the classifier will draw from to assign each input paragraph  ``document'' a single label.

\subsection{Feature Set}

Arguably the single most important part of the document classifier is its feature set, and the quality of the feature(s) chosen for the training and test data can mean the difference between a highly accurate classifier and a faulty one.
To make the most of the data collected earlier (see section~\ref{sec:data-collection}), 24 trials are run, each using a different feature set.
While each of the 24 feature sets are indeed different, they created from all of the possible combinations of three different feature characteristics: case-sensitivity, feature, and sampling technique.

\subsubsection{Case Sensitivity}

Like most texts, both the text in {\it Biology} and from Wikipedia contain a mix of upper and lowercase letters, known as mixed case.
When analyzing mixed case text, one must choose whether to preserve the mixed case or normalize the text to one uniform case.
Choosing to normalize the text means that all analysis that happens will be case-insensitive (case does not matter), and conversely leaving the texts in their original mixed case form means the analysis is case-sensitive (case matters).
Since case-sensitivity has the power to affect a classifier's results, all feature sets are run twice: once case-sensitively, and again case-insensitively.

\subsubsection{Primary Feature}

When creating a feature set, one must decide the criteria by which individual words are selected as features.
The primary feature defines how features are discovered in the text, and this research runs trials with four different primary features.
To make the selection of these features easier to understand, the following slightly modified excerpt from Wikipedia's ``Animal'' article will be used throughout this section as an example of the selection process.\footnote{{\bf Bold} text represents a link to another article as seen by a user, whereas the \textsuperscript{\it superscript} text immediately following bold text represents the title of the article being linked to. This title is not seen by the users until they click the bold link text.}

\begin{quote}
All animals are {\bf heterotrophic beings}\textsuperscript{Heterotroph}, meaning that they feed directly or indirectly on other living things. They are often further subdivided into groups such as {\bf carnivores}\textsuperscript{Carnivore}, {\bf herbivores}\textsuperscript{Herbivore}, {\bf omnivores}\textsuperscript{Omnivore}, and {\bf parasites}\textsuperscript{Parasitic animals}.
\end{quote}

\subsubsubsection{Contains}

The simplest of all features, the {\it contains} feature consists of all the words in the training set.
The contains feature is suggested in Natural Language Processing with Python and Speech and Language Processing as a simple starting point for creating a feature set~\cite{nlpwp,jurafsky}.
If the above paragraph were processed as training data by a \naive Bayes classifier, every word would be selected.\footnote{\underline{Underlines}, both here and throughout this paper, are used to denote feature selection. Each solid underline represents a \underline{single feature}.}
If the training set had 1,000 unique words, it would have 1,000 different ``contains'' features, one for each word.

\begin{quote}
\underline{All} \underline{animals} \underline{are} {\bf \underline{heterotrophic} \underline{beings}}\textsuperscript{Heterotroph}, \underline{meaning} \underline{that} \underline{they} \underline{feed} \underline{directly} \underline{or} \underline{indirectly} \underline{on} \underline{other} \underline{living} \underline{things}. \underline{They} \underline{are} \underline{often} \underline{further} \underline{subdivided} \underline{into} \underline{groups} \underline{such} \underline{as} \underline{\bf carnivores}\textsuperscript{Carnivore}, \underline{\bf herbivores}\textsuperscript{Herbivore}, \underline{\bf omnivores}\textsuperscript{Omnivore}, \underline{and} \underline{\bf parasites}\textsuperscript{Parasitic animals}.
\end{quote}

Many of the underlined words above, like ``things'', ``other'', ``subdivided'', {\it et al.} are not indicative of the word ``Animal''.
One of the drawbacks of the contains feature is how many unhelpful---but nevertheless underlined---words make it into the feature set, since literally every word in the training set is its own feature.
Since features are supposed to communicate information about a text (in this case, features are used to indicate an article's subject), the presence of so many features that do not help indicate the paragraph's subject suggests the {\it contains} primary feature characteristic does not describe an optimal feature set.
Therefore, it makes sense to try some more restrictive feature sets in addition to testing the contains feature.

\subsubsubsection{In First Sentence}

A summary is similar to an index in that they both attempt to summarize a large amount of text into a very small amount of text, so it seemed likely that the sources collected in {\it Speech and Language Processing} for the topic of automatic extractive summarization might also apply to automatic indexing.
Both this and the next primary feature are inspired by the section of Jurafsky and Martin's covering supervised selection of content for extractive summarization.
The section lists several features that can be used to select sentences, ``that are predictive of being a good sentence to appear in a summary''.
Of the five possible features for extractive summarization listed, ``position'' seemed a likely choice for automatic indexing for reasons that will become clear shortly.

Position of a piece of text inside a document can indicate a sentence that summarizes a larger group of text, as in the case of a topic sentence in a paragraph.
Research indicates that the first sentence of a paragraph\footnote{Known as the ``topic sentence'' by many.} is usually a good candidate for use in a summary, and therefore is likely to contain terms that indicate a certain index entry~\cite{jurafsky}.

The first way this research uses position in text to inform feature selection for automatic indexing is by including in the feature set every unique word in the first sentence of each paragraph in the training set.
This means the example training paragraph looks like this when the ``in first sentence'' feature is used:

\begin{quote}
\underline{All} \underline{animals} \underline{are} {\bf \underline{heterotrophic} \underline{beings}}\textsuperscript{Heterotroph}, \underline{meaning} \underline{that} \underline{they} \underline{feed} \underline{directly} \underline{or} \underline{indirectly} \underline{on} \underline{other} \underline{living} \underline{things}. They are often further subdivided into groups such as {\bf carnivores}\textsuperscript{Carnivore}, {\bf herbivores}\textsuperscript{Herbivore}, {\bf omnivores}\textsuperscript{Omnivore}, and {\bf parasites}\textsuperscript{Parasitic animals}.
\end{quote} 

\subsubsubsection{First Word in Sentence}
\label{sec:first-word}

Following from the research on position mentioned above, words may instead be selected by including the first word of each sentence in the feature set.
The inspiration is that many sentences lead off with a proper noun that acts as the topic of the words that follow.
This phenomena is intuitively apparent in the {\it Biology} training data.

Below, the example excerpt is shown with the first word in each paragraph selected for the feature set.
Unfortunately, in this case the two words selected (``All'' and ``They'') are not very indicative of either sentence's topic.
It was predicted that the first word of each sentence would not make for as strong of a feature set as the words in the first sentence of each paragraph.

\begin{quote}
\underline{All} animals are {\bf heterotrophic beings}\textsuperscript{Heterotroph}, meaning that they feed directly or indirectly on other living things. \underline{They} are often further subdivided into groups such as {\bf carnivores}\textsuperscript{Carnivore}, {\bf herbivores}\textsuperscript{Herbivore}, {\bf omnivores}\textsuperscript{Omnivore}, and {\bf parasites}\textsuperscript{Parasitic animals}.
\end{quote}

\subsubsubsection{Linked Article Titles}
\label{subsubsubsec:feature-links}

The above three primary features, while presumed useful, do not make any specific use of the meta data provided by the unique Wikipedia data set.
One of the defining characteristics of a Wikipedia page is the set of textual links, defined by the article's editor(s) to other Wikipedia pages.
These links to other pages are typically only on words that are relevant to the given topic.
E.g., no editor would link to the Wikipedia page on the word ``the'' in an article about beekeeping, but they might link to an article on ``Mesopotamia'', one of the earliest beekeeping civilizations in human history.
This is because the links' purpose is to give background information to readers that will help them understand a concept related to the article they are reading.

Since most---if not all---of the links in Wikipedia are used to provide background information and show relations to other topics, it would seem possible to effectively use the links for developing a supervised classifier.
This research uses this data to create a primary feature by pre-processing the Wikipedia training set, gathering all of the intra-Wiki links using the method described in section~\ref{sec:ranked-titles}, and storing them in a file by most-recent title first.
These titles are used in a {\it contains(title)} feature, where the feature processor records which of the titles a particular article has in text.

For example, the example training paragraph would be pre-processed and the words ``Heterotroph'', ``Carnivore'', ``Herbivore'', ``Omnivore'', and ``Parasitic animals'' would appear in the feature set.
When this same text is processed by the feature processor in the training set, the processor checks whether the text contains any of these five words, in addition to the other words in the feature set.
The processor would find ``Heterotroph'', since the word is inside ``{\bf heterotroph}ic beings'', and it would likewise find ``Carnivore'', ``Herbivore'', and ``Omnivore''.
However, it would not find ``parasites'', because the title of the article the word happens to link to is ``Parasitic animals'', and the latter string of text is nowhere to be found in the original source.
The feature processor cannot look at the title of the article linked to in this excerpt because the test set does not contain links like this, and the feature processor must perform the same work on both sets.

\begin{quote}
All animals are {\bf heterotrophic beings}\textsuperscript{\underline{Heterotroph}}, meaning that they feed directly or indirectly on other living things. They are often further subdivided into groups such as {\bf carnivores}\textsuperscript{\underline{Carnivore}}, {\bf herbivores}\textsuperscript{\underline{Herbivore}}, {\bf omnivores}\textsuperscript{\underline{Omnivore}}, and {\bf parasites}\textsuperscript{\underline{Parasitic animals}}.
\end{quote}

In general, it is hypothesized that this feature will have the best results of all other primary features.
This is because the feature set using this primary feature will contain key concepts and themes relating more directly to the various labels than a simple ``contains feature''.
This is also the only primary feature that allows for multi-word features (see ``Parasitic animals'' above).
It is predicted that this feature will perform best when using a case-insensitive mapping, since all Wikipedia titles begin with a capital letter and the only capital words in either the training or tests sets are words at the beginning of sentences and/or proper nouns.

\subsubsection{Feature Sample Size}
\label{sec:sample-size}

Since the source texts for the training and test sets are so large, applying any one of the primary features listed above generates a large number of features.
When applying the contains feature, for example, the feature processor found 51,261 unique words in the training set.
A number this large would not seem to be a problem for computers today; however, in writing the classifier it was discovered that every 2,000 features required 1.6 gigabytes of memory in order to run and adds approximately 30 minutes to the classifier's run time.
With 51,261 features to select for a case-sensitive contains feature set, the classifier would require approximately 40 gigabytes of memory and would run for over 10 hours before producing a result.
For the computers this research had access to---and the desire for breadth of results---these requirements were considered too high.
To reduce the memory and time requirements, the cardinalities of each the feature set were restricted to 2,000 features each; an acceptable number based on the resources available; however, when reducing a feature set, the criteria used to keep and remove features may affect the results of the classifier.
In the next section, different techniques for selecting a subset of features are discussed.

\subsubsection{Feature Sampling Technique}

When selecting a sample of features to be used from a larger population of features, care must be taken to select only the most useful features.
Usefulness is difficult to determine directly, so one must use indirect methods to mark features for selection.
One way to select features is by looking at how frequently each feature occurs in the training set relative to other features.
Frequency distributions are used to sort features by how frequently they occur.
After this step, features can be selected from the frequency distribution by frequency.
Specifically, this research extracts three groups of features from this distribution:

\begin{itemize}
\item 2,000 most frequently occurring features
\item 2,000 least frequently occurring features
\item 2,000 randomly selected features
\end{itemize}

The group of randomly selected features was included as a control for the most and least frequently occurring features, since random selection would create a subset of features most representative of the original set.

\subsection{Conducting the Experiment}

After gathering the data and establishing the different feature characteristic combinations to, the next step was writing the classifier.
After the classifier was written, it was installed on a number of computers and tested with all 24 unique combinations of feature characteristics listed below in Table~\ref{tab:feature-characteristics}. 

\begin{center}
\begin{table}[H]
\begin{minipage}[t]{.33\linewidth}
\vspace{0pt}
\centering
\begin{tabular}{l}
\textbf{Case-Sensitivity} \\
\hline
Case-sensitive \\
Case-insensitive \\
\end{tabular}
\end{minipage}\hfill
\begin{minipage}[t]{.33\linewidth}
\vspace{0pt}
\centering
\begin{tabular}{l}
\textbf{Primary Feature} \\
\hline
Contains \\
In first sentence \\
First word in sentence \\
Linked article titles \\
\end{tabular}
\end{minipage}\hfill
\begin{minipage}[t]{.33\linewidth}
\vspace{0pt}
\centering
\begin{tabular}{l}
\textbf{Sampling Technique} \\
\hline
Most frequent \\
Least frequent \\
Random \\
\end{tabular}
\end{minipage}
\caption{Comprehensive list of feature characteristics being used.\label{tab:feature-characteristics}}
\end{table}
\end{center}

\subsubsection{Writing the Classifier}

The classifier is implemented in Python and uses the Natural Language Toolkit library (discussed in section~\ref{sec:nltk}).
classifier.py takes the inputs of a directory containing plain text Wikipedia articles extracted in section~\ref{subsubsec:RDB2F}, the directory of plain text {\it Biology} paragraphs (\ref{sec:training-files}), rankedTitles.txt (\ref{sec:ranked-titles}) and the indexToWiki.json map of index labels to Wikipedia page titles (\ref{sec:indexToWiki}).
With these inputs, the script trains a \naive Bayes classifier (imported from NLTK) using the combination of feature characteristics is configured to use at the bottom of the program file.
Once trained, the program tests the classifier against the {\it Biology} test set.
Once testing is complete, the program outputs the combination of feature characteristics used to create the feature set, followed by a fractional percentage representing the classifier's accuracy using NLTK's {\it classify} module and the test set.
The code for classifier.py is shown in appendix~\ref{appendix:b}.

\subsubsection{Running the Classifier}

The classifier was run on twelve commodity HP computers with 64-bit Windows~7, Intel~i7 processors, and 4~gigabytes of memory installed.
A zip file containing classifier.py, plain text Wikipedia data, plain text {\it Biology} data, indexToWiki.json, and rankedTitles.txt was copied twice\footnote{The processor would have allowed up to four instances of the classifier to run simultaneously (one on each core), but the classifier's large memory requirements meant that only two instances of the classifier program could run comfortably on each computer.} onto each of the computers.
Each copy of the classifier was then configured to use a unique combination of feature characteristics defining the contents of the feature set.
Finally, both copies of classify.py were ran simultaneously on each computer, taking advantage of the processors' multiple cores.
When each process completed execution, the terminated process's feature characteristics and accuracy were noted in a spreadsheet.

\subsection{Experimental Results and Discussion}

The experiment's 24 trials and their results are summarized in tables \ref{tab:results-grouped} and \ref{tab:results-sorted} below.
The former presents the experiment's results grouped by feature characteristic, making it possible to easily locate and compare the effectiveness between features with many similar feature characteristics.
The latter organizes the experimental results by accuracy, with the most accurate features on top.

Table~\ref{tab:results-grouped} shows, that generally, case-insensitive matches are more accurate than case-sensitive matches (by about double).
The best case-sensitive feature combines the {\it most frequent} and {\it in first sentence} feature characteristics, while the best case-insensitive feature (and best overall feature) combines {\it linked article titles} and {\it most frequent}.
The table also shows that the {\it contains} feature yields the worst results of all primary features, with a peak of 0.40\% accuracy when it is used in combination with the {\it random} and {\it case-insensitive} feature characteristics.
This is to be expected, since the other three primary features use heuristics to select only individual features that are more likely to have relevance to the entire paragraph than a random word in the text.

Table~\ref{tab:results-sorted} shows the general success of the various features relative to the next most and next least accurate feature.
From this table, it is clear that the best feature is significantly more accurate than any other feature with an accuracy of 9.49\%.
The next best method is {\it case-insensitive, in first sentence, most frequent} at 3.03\%.
After the top two, the two case-sensitive versions of these features follow at 1.21\% and 1.82\% respectively.

Visibly, Table~\ref{tab:results-sorted} also reveals a trend in sampling technique that is worthy of discussion.
The table lists most frequent as almost unanimously the most effective sampling technique, with all but two of the features using the {\it most frequent} feature characteristic at the top of the table.
The other two features using {\it most frequent} are at the bottom of the table, but these features both use the relatively unreliable {\it contains} primary feature.
{\it Most frequent} was probably the worst of the feature using {\it contains} because stop words\footnote{Stop words are words that, while important for grammar, are largely irrelevant to the document. These are words like ``the'', ``and'', and ``have''.} were not removed for this research, meaning the {\it most-frequent contains} feature likely included a large number of stop words within the 2,000 feature used.

Like {\it contains}, {\it first word in sentence} appears largely ineffective with a peak accuracy of 0.61\% in case-sensitive and case-insensitive {\it most frequent}.
Given the example provided in section~\ref{sec:first-word}, this is not entirely surprising.
Even though the sentences in the {\it Biology} data set frequently begin with informative words, the same does not appear to have held true for the Wikipedia data set.
This is a consequence of using training and test sets from different sources---the writing style is not always similar between the two sets.

\pagebreak
\begin{center}
\begin{table}[h]
\caption{Indexing results using 2,000 features.}
\begin{tabular}{cllll}
\multicolumn{1}{l}{\textbf{\begin{tabular}[c]{@{}c@{}}\label{tab:results-grouped}Sensitivity\end{tabular}}} & \textbf{Primary Feature} & \textbf{Sampling Technique} & \textbf{Accuracy} \\ \hline
\multirow{12}{*}{Case-sensitive}   & \multirow{3}{*}{Contains}               & Least frequent              & 0.20\%    \\ \cline{3-4} 
\multicolumn{1}{l}{}                                    &                                         & Most frequent               & 0.13\%  \\ \cline{3-4} 
\multicolumn{1}{l}{}                                    &                                         & Random                      & 0.20\%  \\ \cline{2-4} 
\multicolumn{1}{l}{}                                    & \multirow{3}{*}{In first sentence}      & Least frequent              & 0.20\%  \\ \cline{3-4} 
\multicolumn{1}{l}{}                                    &                                         & Most frequent               & 1.82\%  \\ \cline{3-4} 
\multicolumn{1}{l}{}                                    &                                         & Random                      & 0.20\%  \\ \cline{2-4} 
\multicolumn{1}{l}{}                                    & \multirow{3}{*}{First word in sentence} & Least frequent              & 0.20\%  \\ \cline{3-4} 
\multicolumn{1}{l}{}                                    &                                         & Most frequent               & 0.61\%  \\ \cline{3-4} 
\multicolumn{1}{l}{}                                    &                                         & Random                      & 0.40\%  \\ \cline{2-4} 
\multicolumn{1}{l}{}                                    & \multirow{3}{*}{Linked article titles}  & Least frequent              & 0.20\%  \\ \cline{3-4} 
\multicolumn{1}{l}{}                                    &                                         & Most frequent               & 1.21\%  \\ \cline{3-4} 
\multicolumn{1}{l}{}                                    &                                         & Random                      & 0.40\%  \\ \cline{1-4} 
\multicolumn{1}{l}{} \multirow{12}{*}{Case-insensitive} & \multirow{3}{*}{Contains}               & Least frequent              & 0.20\%  \\ \cline{3-4} 
\multicolumn{1}{l}{}                                    &                                         & Most frequent               & 0.14\%  \\ \cline{3-4} 
\multicolumn{1}{l}{}                                    &                                         & Random                      & 0.40\%  \\ \cline{2-4} 
\multicolumn{1}{l}{}                                    & \multirow{3}{*}{In first sentence}      & Least frequent              & 0.20\%  \\ \cline{3-4} 
\multicolumn{1}{l}{}                                    &                                         & Most frequent               & 3.03\%  \\ \cline{3-4} 
\multicolumn{1}{l}{}                                    &                                         & Random                      & 0.20\%  \\ \cline{2-4} 
\multicolumn{1}{l}{}                                    & \multirow{3}{*}{First word in sentence} & Least frequent              & 0.20\%  \\ \cline{3-4} 
\multicolumn{1}{l}{}                                    &                                         & Most frequent               & 0.61\%  \\ \cline{3-4} 
\multicolumn{1}{l}{}                                    &                                         & Random                      & 0.40\%  \\ \cline{2-4} 
\multicolumn{1}{l}{}                                    & \multirow{3}{*}{Linked article titles}  & Least frequent              & 0.40\%  \\ \cline{3-4} 
\multicolumn{1}{l}{}                                    &                                         & Most frequent               & 9.49\%  \\ \cline{3-4} 
\multicolumn{1}{l}{}									   &                                         & Random                      & 0.40\%  \\ \hline
\end{tabular}
\end{table}
\end{center}

\pagebreak
\begin{center}
\begin{table}[h]
\caption{Indexing results using 2,000 features, sorted by accuracy, descending.}
\begin{tabular}{llll}
\label{tab:results-sorted}
\textbf{Sensitivity} & \textbf{Primary Feature}       & \textbf{Sampling Technique} & \textbf{Accuracy $\downarrow$} \\ \hline
Case-insensitive     & Linked article titles  & Most frequent               & 9.49\%             \\ \hline
Case-insensitive     & In first sentence      & Most frequent               & 3.03\%             \\ \hline
Case-sensitive       & In first sentence      & Most frequent               & 1.82\%             \\ \hline
Case-sensitive       & Linked article titles  & Most frequent               & 1.21\%             \\ \hline
Case-sensitive       & First word in sentence & Most frequent               & 0.61\%             \\ \hline
Case-insensitive     & First word in sentence & Most frequent               & 0.61\%             \\ \hline
Case-sensitive       & First word in sentence & Random                      & 0.40\%             \\ \hline
Case-sensitive       & Linked article titles  & Random                      & 0.40\%             \\ \hline
Case-insensitive     & Contains               & Random                      & 0.40\%             \\ \hline
Case-insensitive     & First word in sentence & Random                      & 0.40\%             \\ \hline
Case-insensitive     & Linked article titles  & Random                      & 0.40\%             \\ \hline
Case-insensitive     & Linked article titles  & Least frequent              & 0.40\%             \\ \hline
Case-sensitive       & Contains               & Random                      & 0.20\%             \\ \hline
Case-sensitive       & Contains               & Least frequent              & 0.20\%             \\ \hline
Case-sensitive       & In first sentence      & Random                      & 0.20\%             \\ \hline
Case-sensitive       & In first sentence      & Least frequent              & 0.20\%             \\ \hline
Case-sensitive       & First word in sentence & Least frequent              & 0.20\%             \\ \hline
Case-sensitive       & Linked article titles  & Least frequent              & 0.20\%             \\ \hline
Case-insensitive     & Contains               & Least frequent              & 0.20\%             \\ \hline
Case-insensitive     & In first sentence      & Random                      & 0.20\%             \\ \hline
Case-insensitive     & In first sentence      & Least frequent              & 0.20\%             \\ \hline
Case-insensitive     & First word in sentence & Least frequent              & 0.20\%             \\ \hline
Case-insensitive     & Contains               & Most frequent               & 0.14\%             \\ \hline
Case-sensitive       & Contains               & Most frequent               & 0.13\%             \\ \hline
\end{tabular}
\end{table}
\end{center}%0.48 %1.0




% High dropoff from highest to lowest feature
% Discuss trend of most frequent dominant -> random -> least frequent

