\section{Data Collection}
% TODO: Explain indexToWiki table entry (possibly done already, verify.
% TODO: Explain indexToWiki.json (formed by hand from SQL csv output to json) `SELECT indexTitle, wikiTitle FROM thesis.indexToWiki;`
% TODO: Explain rankedTitles.txt

This research leverages supervised learning algorithms, meaning it requires a training set and a test set to develop an appropriate feature set to use in conjunction with the supervised learning algorithm.
The training set allows the algorithm to ``learn'' how to classify different sections of text, and the test set is used to validate that documents are classified correctly.

Classically, training and test data come from the same corpus, the same set of data\cite{jurafsky}.
This research takes test and training set data from two different, mutually exclusive corpora.
The training set is composed of paragraphs from Wikipedia articles, with each paragraph labeled by the title of the article it belongs to.
The test data contain paragraphs from the OpenStax {\it Biology} textbook\cite{biology}, with each paragraph labeled by the index entry that refers to it\footnote{If a {\it Biology} paragraph is not referenced in the index, it is not included in the data set. If it is referenced by multiple index entries, the paragraph is included once per index entry.}

Due to the sheer size of Wikipedia and the memory and computational power available for this research, the data set was reduced by selecting only paragraphs from Wikipedia articles whose title matched in index entry in {\it Biology}.
Likewise, this research makes an assumption that the set of all reasonable index entries for any textbook is the same as the set of all titles in the English Wikipedia.
This assumption lends itself to selecting only paragraphs from {\it Biology} whose labels are the same as Wikipedia article titles.

The following sections contain justification for the choice of data for each set, and the methods used to structure and collect that data for analysis.

\subsection{Test Set}

Since the goal of this research is to create an index for a book, the test set is generated from a textbook with a comprehensive index section.
This textbook is called {\it Biology} \cite{biology}, and is freely available from OpenStax \cite{openstax-bio} under the Creative Commons Attribution license.
The book was created by six senior contributors that hold professorial positions at prevalent universities, and approximates an average college textbook.
This textbook is 1477 pages long, containing an index of 3118 unique topics (labels) making for 4678 different index entries (references).

In {\it Biology}, all words referred to by index entries are bolded in the text itself. Below is an example of what this looks like (bold in original):

\begin{quote}
Symbiotic relationships, or {\bf symbioses} (plural), are close interactions between individuals of different species over an extended period of time which impact the abundance and distribution of the associating populations. \cite{biology}
\end{quote}

\noindent Here, the bold word ``symbioses'' is referred to by an index entry at the back of the book with the same name. All index entries point to bold words, and all bold words point to index entries. This fact makes it trivial to find which part of the page an index reference is referring to, making it a good candidate for test data.

\subsubsection{Structuring the Test Data}

Useful though {\it Biology} is as a data source, it is not available in a structured, easily parsable format.
In order to make the data in {\it Biology} usable, it needs to be converted from its source PDF into a structured form.
A relational database is used to store the book's text so that it can be queried for its contents, paragraph location, page number, and other helpful attributes.

\subsubsection{Importing Index Entries into Relational Database}

To expedite the structuring process, text was copied manually from the PDF, and pasted into a plain text document.
Regular expressions were used to massage the data into a simple, comma separated format.
In this format, each line represents a unique index entry, with the first column in a line holding the index label (or name), and each subsequent column holds a page number the label can be found on.
Now that the data are more easily parsable, there must be a way to it in a relational database for analysis purposes.

A table is created to hold the index labels separate from the page numbers, since one label can refer to multiple pages.
In this table, indexId is an autoincrementing ID, label is the name of the index entry as it appears in the book (e.g., ``Acid rain''), and wikiLabel is the label if the label were a wikipedia article name (e.g., ``Acid\_rain'') which is achieved by replacing spaces in the label with underscores.

\begin{center}
\begin{tabular}{|c|}
\hline 
\textbf{index} \\ 
\hline 
bookId \\ 
\hline 
indexId \\ 
\hline 
label \\ 
\hline 
wikiLabel \\ 
\hline 
\end{tabular}
\end{center}
 
An indexedPage table was created to store index references (page numbers) that belong to index labels.
This table simply contains an indexId and a pageNum, which allows for joins onto the index table to replicate the whole index.

\begin{center}
\begin{tabular}{|c|}
\hline 
\textbf{indexedPage} \\ 
\hline 
indexId \\ 
\hline 
pageNum \\ 
\hline 
\end{tabular} 
\end{center}

Once these tables were created, the structured CSV file was imported using the custom indexImporter.php (see Appendix B) tool written specifically for this purpose.

The importer script populates all of the columns in both tables with all of the data in the CSV file.

\subsubsection{Reducing Index Entry Set}
\label{subsec:reducing}

Only index entries whose {\tt wikiLabel} value matches a Wikipedia article title will be used in this research.
This means that information need only be gathered from index entries that match this criteria.
To discover this subset of index entries, a database of Wikipedia titles must be intersected with the {\tt index}.

The Wikimedia Foundation periodically creates dumps for their many databases and makes them publically available online\cite{wiki-dumps}.
One of the many data sets they make available is a list of Wikipedia article titles in the main \url{/wiki/} namespace for the English language version of Wikipedia\cite{wiki-dump-titles}.
At the time of this writing, there are 10639771 separate Wikipedia article titles matching this criteria.
This dump will serve as the source that will ultimately be intersected with the {\it Biology} index entries to yield the entries that will be used in analyses.

Before performing this intersection, the English Wikipedia title information must be extracted from the dump file and placed in a table in the relational database.
This table contains a unique {\tt titleId} integer key and a textual {\tt title} in each row.

\begin{center}
\begin{tabular}{|c|}
\hline 
{\bf articleTitles} \\ 
\hline 
titleId \\ 
\hline 
title \\ 
\hline 
\end{tabular} 
\end{center}

This data is imported into the {\tt articleTitles} table using the {\tt titleImporter.php} script (see Appendix C).
Now that both the index labels and article titles exist in the relational database, the two datasets can be intersected by running the following MySQL command, where {\tt BINARY} requires case sensitive matching:

\begin{lstlisting}[language=SQL]
SELECT i.wikiLabel
    FROM `index` i, `articleTitles` at
    WHERE BINARY i.wikiLabel = at.title;
\end{lstlisting}

The 3118 unique index labels intersected against all 10639771 Wikipedia articles yields a total number of 518 overlapping terms.
This number would be larger, however all Wikipedia titles begin with a capital letter, and since this was a {\tt BINARY} exact case match all index lables with lowercase initial letters were excluded from the intersection.
This does not seem to introduce a bias towards proper nouns, however, as {\it Biology}'s index contains words and phrases exactly as they appear in the text.
Below is a random sampling of 15 labels from the intersection as they appear in {\it Biology}'s index.
Notice how very few of these labels are proper nouns.

\begin{multicols}{3}
\begin{verbatim}
Bacteriophages
Chordata
Elevation
Exotic species
FtsZ
Heritability
Phloem
Pongo
RNAs
Runners
Schwann cell
S-layer
Southern blotting
Sutural bones
Topoisomerase
\end{verbatim}
\end{multicols}

\subsubsection{Storing the Indexed Text}

Now that the relevant set of index labels have been ascertained, the context in which these labels appear in {\it Biology}'s text must be stored.
As mentioned above, this research defines an index entry's context as the paragraph the indexed word appears in.
In the relational database, two tables exist to hold this information:

\begin{center}
\begin{tabular}{|c|}
\hline 
\textbf{paragraph} \\ 
\hline 
paraNum \\ 
\hline 
bookId \\ 
\hline 
pageNum \\ 
\hline 
endPageNum \\ 
\hline 
body \\ 
\hline 
\end{tabular}
\end{center}

\begin{center}
\begin{tabular}{|c|}
\hline 
\textbf{indexedParagraph} \\ 
\hline 
indexId \\ 
\hline 
paraNum \\ 
\hline 
\end{tabular} 
\end{center}

The contents of a paragraph are stored in the {\tt paragraph} table's body, along with information about the paragraph's location.
{\tt pageNum} and {\tt endPageNum} specify the pages on which the paragraph starts and ends, since a paragraph may continue onto another page.

To associate these paragraphs with the index labels in the {\tt index} table, the {\tt indexedParagraph} table was creted.
This lookup table specifies a many to many relationship between {\tt paragraph} and {\tt index}, since an index entry refers to at least one paragraph, and a paragraph may contain zero or more index references. Once these tables are created, it's time to create the indexed

\subsubsection{Collecting the Indexed Text}

The paragraphs were imported into the relational database using paragraphImporter.php, a small webpage script written specifically for this purpose, allowing a researcher to copy a paragraph into a textbox on that page and fill in the meta data for that paragraph and keep track of the index entries that point to it.

As mentioned above, {\it Biology} comes in PDF form, making it difficult for a computer to parse the individual paragraphs out.
By using the SQL query below, a list of index entries with the pages on which they appear.

\begin{lstlisting}
SELECT DISTINCT ip.pageNum, i.`label`
    FROM thesis.`index` i,
         thesis.indexToWiki iw,
         thesis.indexedPage ip
    WHERE BINARY i.`label` = iw.indexTitle
        AND ip.indexId = i.indexId
        ORDER BY ip.pageNum ASC;
\end{lstlisting}

This list is used to guide data entry, since it contains only the index references which are relevant for this research.
Here is an example of what this list looks like:

\begin{multicols}{2}
\begin{verbatim}
14,Science
16,"Deductive reasoning"
16,"Inductive reasoning"
20,"Basic science"
23,"Review articles"
27,Organs
27,Prokaryotes
28,Organisms
32,Microbiology
33,Zoology
\end{verbatim}
\end{multicols}

Using this list and a PDF viewer, a paragraph is located that corresponds to each line, the text is selected using the cursor, and copied from the PDF to paragraphImporter.php.
Finding which paragraph on a page corresponds to a given index label is a trivial task, as the label always appears bolded in the paragraph it belongs to.

\subsubsection{Relational Database to Files}
\label{subsubsec:RDB2F}

This research uses Python's Natural Language Toolkit (NLTK) library for analysis \cite{nltk}.
With NLTK, it is easier to work with raw files without interacting with a database.
In order to do this, the content that was inserted into the database in the above step needs to be extracted.
The extraction is acomplished using saveIndexParagraphs.php, a script which connects to the database and places paragraphs in files named by their respective index labels.
All of the paragraphs a label refers to are appended to the same file, using two newline characters ({\tt{\textbackslash}n{\textbackslash}n}) as a delimiter between paragraphs.
It is also important to note that a paragraph will be placed in multiple files if multiple labels refer to it.
Now the test set is considered processed, it is time to move on to the training set.

\subsection{Training Set}

The training set is created from a list of Wikipedia articles whose titles are also index labels in {\it Biology} (see~\ref{subsec:reducing}).
Since this list contains only titles, the contents of the article need to be fetched from Wikipedia itself.

\subsubsection{Downloading the Training Data}

Since there are 518 articles to fetch from Wikipedia, downloading each one by one would take time and go against Wikipedia's terms of service. Wikipedia does not allow the use of automated web crawlers when it can be avoided\cite{wiki-robots}.

Instead, the articles were fetched using the Wikipedia page exporter \cite{wiki-exporter}, which produces an XML database dump file.
This file contains the contents for all of the articles specified in the web form, plus extra information about the article like title and date of last edit.
The contents of the article are stored in the MediaWiki markup language \cite{mediawiki-markup}.
This language works similarly to other markup languages like HTML and Markdown, and is used everywhere on Wikipedia.
Using the page exporter makes it possible to circumvent Wikipedia's rules about crawling the website because the export happens in one bulk data transfer rather than the many very small and frequent transfers a web crawler would make.

Wikipedia content editors must use the MediaWiki language to edit pages. 
While MediaWiki is great for writing and editing content, it is difficult for computers to directly query text written in the language.
To solve this problem, the MediaWiki format must be converted to HTML.
HTML has the useful {\tt <p>} tag, which is used to distinguish paragraphs from one another, a requirement for this research. 
Unfortunately, the only full-featured MediaWiki to HTML converter is part of the MediaWiki platform itself.
A computer must be made into a mini-Wikipedia by installing MediaWiki on it and importing data exported from Wikipedia itself.

\subsubsection{Converting MediaWiki Markup to HTML}

To convert articles to HTML, a local MediaWiki server was installed on a personal computer using MediaWiki's online instructions \cite{mediawiki-installation} with the purpose of creating a local version of Wikipedia that only contains the articles that share titles with index labels from {\bf Biology}.
This mini-Wikipedia can then be crawled using printPageLinks.py to extract and store the HTML versions of the articles.

After installation, all available first-party MediaWiki plugins were enabled in the installation, as Wikipedia uses many of them on their own MediaWiki installation.
Once installed, the database import tool \cite{mediawiki-import} is used to import the XML file from Wikipedia's database dump into the local MediaWiki installation, creating a small version of Wikipedia containing only the articles needed for this research.
The MediaWiki platform takes care of converting MediaWiki markup into HTML when it receives an HTTP request for a given article, just like with regular Wikipedia.

\subsubsection{Finding Redirects}

Inspection of the articles imported into the local MediaWiki installation showed that a large number of the articles are actually just redirect pages to a complete article with a different name.
The Wikipedia exporter does not provide full pages the redirect pages reference.
There is, however, a way to obtain a list of the full-text files.
Here is an example of what a redirect page looks like in the XML database dump:

\begin{lstlisting}
<page>
  <title>5' cap</title>
  <ns>0</ns>
  <id>37542765</id>
  <redirect title="Five prime cap" />
  <revision>
    <id>528722171</id>
    <parentid>521385832</parentid>
    <timestamp>2012-12-19T00:45:42Z</timestamp>
    <contributor>
      <username>MZMcBride</username>
      <id>212624</id>
    </contributor>
    <minor/>
    <comment>[[bugzilla:42616]]</comment>
    <text xml:space="preserve" bytes="28">
        #REDIRECT [[Five prime cap]]
    </text>
    <sha1>kzecsgfge2ind0a0652k24ldw1unbgd</sha1>
    <model>wikitext</model>
    <format>text/x-wiki</format>
  </revision>
</page>
\end{lstlisting}

Line 5 in the above sample shows a redirect tag that contains the title of the complete article this page redirects to.
These redirect tags were discovered by using findRedirects.py.
This script does two things.
First, it changes the {\tt wikiTitle} field in the {\tt indexToWiki} table from the title of the redirect page (``$5^\prime$ cap'' in the above example) to the title of the page it redirects to (``Five prime cap``).
Second, it prints out the wikified title (``Five prime cap'' becomes ``Five\_prime\_cap``) on each line of output.
The results of the program's output is then copy-pasted back into the Wikipedia exporter \cite{wiki-exporter}, yielding a new XML database dump full of the complete articles.
This XML file is then imported into the MediaWiki site, giving the redirect pages their respective targets to point to.

The process of resolving redirects did not need to be done on the second XML database dump because it only contained full articles (no redirects to other pages).

\subsubsection{Saving Pages as HTML}

Since all necessary articles have been imported into the local MediaWiki server, it is now possible to download them all in HTML.
To do this, a list of links to each of the articles imported into the MediaWiki server is required.
MediaWiki provides this list via the {\tt Special:AllPages} family of pages.
printPageLinks.py is used to locate all of the links on these special pages and download all non-redirect articles in HTML.
The Python script saves each article in a file with the same title as the article itself.

\subsubsection{Extracting Paragraphs from Article HTML}

Now that the articles are in HTML, the paragraphs need to be extracted from them and stored in the same format as the {\it Biology} paragraphs from section~\ref{subsubsec:RDB2F}.
Another Python tool---html2text.py---uses BeautifulSoup \cite{beautifulsoup} to extract the contents of all {\tt <p>} tags from the body of each HTML Wikipedia article.
The script appends each paragraph to a file (one file per article) with each paragraph in the file separated by two newline characters as in section~\ref{subsubsec:RDB2F}.

\subsubsection{Dealing with Disambiguation Pages}

Redirect pages aren't articles in and of themselves, so they were removed from the training set.
However, there is also another type of article whose content does not accurately represent a specific concept, and those are disambiguation pages.

\begin{quote}
Disambiguation in Wikipedia is the process of resolving the conflicts that arise when a single term is ambiguous---when it refers to more than one topic covered by Wikipedia. \cite{wiki-disambiguation}
\end{quote}

A disambiguation page differs in content from a redirect page in two primary ways: disambiguation pages do not follow a specific structure and always link to more than one page.
Since the subjects of disambiguation pages are, by nature, multi-topic, they should not be used for this research.

Disambiguation pages are difficult to locate and remove because they do not follow a specific structure, although they are fairly obvious to the human eye.
Thus, disambiguation pages were removed with the help of cullDisambiguation.sh, a script which displays an article to the user and asks them whether it should be kept or deleted.

Although there was not a deterministic process guiding script, articles were only deleted if they contained sentences in the article ``abstract'' following the patterns, ``X commonly refers to,'' ``Y may also refer to'' or similar. Articles are also considered to be disambiguation pages if they contained unordered lists of links to similarly named articles.

\subsubsection{Removing Spurious Article Text}

Several of the plain text paragraph files contain spurious ``paragraphs'' at the end of the file which are either ``Cite error''s or links to view the article in a different language.
Neither of these traits are relevant to the article, so they can be safely removed.
Unlike disambiguation pages, locating and removing these errors followed a deterministic, repeatable process.
The offending pages are located using the pattern matching script cullSpuriousArticleText.sh hand the offending lines were removed by hand.
Manually deleting the spurious lines prevented false positive matches from being removed from the articles, since this text could conceivably (however unlikely) appear in-context within the text of an article.

\subsection{Discovering Most Commonly Linked Articles}

TODO: Explain extractPageLinks.py's creation of rankedTitles.tsv and conversion of rankedTitles.tsv to rankedTitles.txt here. Share information about most popular linked titles.
